{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T18:55:07.326980Z",
     "start_time": "2020-07-04T18:55:06.140076Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixed download web page images code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:21:46.397986Z",
     "start_time": "2020-07-06T22:21:46.321881Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "def get_all_images(url):\n",
    "    \"\"\"\n",
    "    Returns all image URLs on a single `url`\n",
    "    \"\"\"\n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    urls = []\n",
    "    for img in tqdm(soup.find_all(\"img\"), \"Extracting images\"):\n",
    "        img_url = img.attrs.get(\"src\")\n",
    "        if not img_url:\n",
    "            # if img does not contain src attribute, just skip\n",
    "            continue\n",
    "        # make the URL absolute by joining domain with the URL that is just extracted\n",
    "        img_url = urljoin(url, img_url)\n",
    "        # remove URLs like '/hsts-pixel.gif?c=3.2.5'\n",
    "        try:\n",
    "            pos = img_url.index(\"?\")\n",
    "            img_url = img_url[:pos]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        # finally, if the url is valid\n",
    "        if is_valid(img_url):\n",
    "            urls.append(img_url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def download(url, pathname):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "    # get the file name\n",
    "    filename = os.path.join(pathname, url.split(\"/\")[-1])\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", \n",
    "    unit_scale=True, unit_divisor=1024)\n",
    "    \n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress:\n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:21:48.360810Z",
     "start_time": "2020-07-06T22:21:48.354446Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(url, path):\n",
    "    # get all images\n",
    "    imgs = get_all_images(url)\n",
    "    for img in imgs:\n",
    "        # for each img, download it\n",
    "        download(img, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"This script downloads all images from a web page\")\n",
    "    parser.add_argument(\"url\", help=\"The URL of the web page you want to download images\")\n",
    "    parser.add_argument(\"-p\", \"--path\", help=\"The Directory you want to store your images, default is the domain of URL passed\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    url = args.url\n",
    "    path = args.path\n",
    "\n",
    "    if not path:\n",
    "        # if path isn't specified, use the domain name of that url as the folder name\n",
    "        path = urlparse(url).netloc\n",
    "    \n",
    "    main(url, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T19:04:46.560323Z",
     "start_time": "2020-07-04T19:04:46.557061Z"
    }
   },
   "source": [
    "img_url='https://imgcrack.com/story/225/pichana-yoosuk'\n",
    "img_dir='/Users/brettlv/Documents/pichana-yoosuk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## youtube list pichana-yoosuk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://www.youtube.com/watch?v=dRchMA6no0A\n",
    "    \n",
    "https://www.youtube.com/watch?v=zDdM8wq09TI&list=PLA3FtL0KxkNJO4oei1-8QpVkzfVDbCCUM    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T01:58:41.251697Z",
     "start_time": "2020-07-05T01:56:09.831445Z"
    }
   },
   "source": [
    "img_url='https://xhamster.com/photos/gallery/pichana-yoosuk-13243625'\n",
    "img_dir='/Users/brettlv/Downloads/books/sss/pichana-yoosuk/sss/pichana-yoosuk'\n",
    "main(img_url, img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21 position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T00:56:30.709158Z",
     "start_time": "2020-07-05T00:55:34.707171Z"
    }
   },
   "source": [
    "img_url='https://lovefindsitsway.com/best-sex-positions-for-couples/'\n",
    "img_dir='/Users/brettlv/Downloads/books/sss/21_position'\n",
    "main(img_url, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T01:33:37.531819Z",
     "start_time": "2020-07-05T01:32:51.571014Z"
    }
   },
   "source": [
    "img_url='http://www.cloudysexy.com/porn/liu-yi-fei-naked-photos-excellent-porn.html'\n",
    "img_dir='/Users/brettlv/Downloads/books/sss/liuyifei'\n",
    "main(img_url, img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T01:10:39.826473Z",
     "start_time": "2020-07-05T01:09:40.692243Z"
    }
   },
   "source": [
    "img_url='https://www.schoolofsquirt.com/50-best-sex-positions/#The_Best_Oral_Sex_Positions_For_Both_of_You'\n",
    "img_dir='/Users/brettlv/Downloads/books/sss/50_position/69/'\n",
    "main(img_url, img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T01:04:38.469711Z",
     "start_time": "2020-07-05T01:03:38.367472Z"
    }
   },
   "source": [
    "img_url='https://www.schoolofsquirt.com/50-best-sex-positions/'\n",
    "img_dir='/Users/brettlv/Downloads/books/sss/50_position'\n",
    "main(img_url, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## dingding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "img_url='https://wemp.app/posts/588ea3d2-a0bc-493a-8543-552dd07afede' \n",
    "img_dir='/Users/brettlv/Downloads/books/sss/丁丁/' \n",
    "main(img_url, img_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:22:10.807171Z",
     "start_time": "2020-07-06T22:21:57.797834Z"
    }
   },
   "source": [
    "img_url='https://www.gq.com.tw/entertainment/article/%E9%9F%93%E5%9C%8B%E6%AD%A3%E5%A6%B9-%E5%81%A5%E8%BA%AB-%E5%BD%BC%E6%8B%89%E6%8F%90%E6%96%AF-%E7%94%B3%E6%99%BA%E7%A7%80'\n",
    "img_dir='/Users/brettlv/Downloads/books/sss/sss_image/申智秀/'\n",
    "main(img_url, img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# added download images from javascript-driven websites script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:19:28.720570Z",
     "start_time": "2020-07-06T15:19:27.515871Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_images(url):\n",
    "    \"\"\"\n",
    "    Returns all image URLs on a single `url`\n",
    "    \"\"\"\n",
    "    # initialize the session\n",
    "    session = HTMLSession()\n",
    "    # make the HTTP request and retrieve response\n",
    "    response = session.get(url)\n",
    "    # execute Javascript\n",
    "    response.html.render()\n",
    "    # construct the soup parser\n",
    "    soup = bs(response.html.html, \"html.parser\")\n",
    "    urls = []\n",
    "    for img in tqdm(soup.find_all(\"img\"), \"Extracting images\"):\n",
    "        img_url = img.attrs.get(\"src\") or img.attrs.get(\"data-src\")\n",
    "        if not img_url:\n",
    "            # if img does not contain src attribute, just skip\n",
    "            continue\n",
    "        # make the URL absolute by joining domain with the URL that is just extracted\n",
    "        img_url = urljoin(url, img_url)\n",
    "        # remove URLs like '/hsts-pixel.gif?c=3.2.5'\n",
    "        try:\n",
    "            pos = img_url.index(\"?\")\n",
    "            img_url = img_url[:pos]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        # finally, if the url is valid\n",
    "        if is_valid(img_url):\n",
    "            urls.append(img_url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def download(url, pathname):\n",
    "    \"\"\"\n",
    "    Downloads a file given an URL and puts it in the folder `pathname`\n",
    "    \"\"\"\n",
    "    # if path doesn't exist, make that path dir\n",
    "    if not os.path.isdir(pathname):\n",
    "        os.makedirs(pathname)\n",
    "    # download the body of response by chunk, not immediately\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # get the total file size\n",
    "    file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "    # get the file name\n",
    "    filename = os.path.join(pathname, url.split(\"/\")[-1])\n",
    "\n",
    "    # progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "    progress = tqdm(response.iter_content(1024), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for data in progress:\n",
    "            # write data read to the file\n",
    "            f.write(data)\n",
    "            # update the progress bar manually\n",
    "            progress.update(len(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:19:57.487807Z",
     "start_time": "2020-07-06T15:19:57.482847Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_js(url, path):\n",
    "    # get all images\n",
    "    imgs = get_all_images(url)\n",
    "    for img in imgs:\n",
    "        # for each img, download it\n",
    "        download(img, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T15:20:12.150933Z",
     "start_time": "2020-07-06T15:19:59.905742Z"
    },
    "collapsed": false
   },
   "source": [
    "img_url='https://wemp.app/posts/588ea3d2-a0bc-493a-8543-552dd07afede' \n",
    "img_dir='/Users/brettlv/Downloads/books/sss/丁丁/' \n",
    "main_js(img_url, img_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_js(url, path):\n",
    "    # get all images\n",
    "    imgs = get_all_images(url)\n",
    "    for img in imgs:\n",
    "        # for each img, download it\n",
    "        download(img, path)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"This script downloads all images from a web page\")\n",
    "    parser.add_argument(\"url\", help=\"The URL of the web page you want to download images\")\n",
    "    parser.add_argument(\"-p\", \"--path\", help=\"The Directory you want to store your images, default is the domain of URL passed\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    url = args.url\n",
    "    path = args.path\n",
    "\n",
    "    if not path:\n",
    "        # if path isn't specified, use the domain name of that url as the folder name\n",
    "        path = urlparse(url).netloc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main(url, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# the url of file you want to download, passed from command line arguments\n",
    "#url = sys.argv[1]\n",
    "\n",
    "url=''\n",
    "# read 1024 bytes every time \n",
    "buffer_size = 1024\n",
    "# download the body of response by chunk, not immediately\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# get the total file size\n",
    "file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "# get the file name\n",
    "filename = url.split(\"/\")[-1]\n",
    "\n",
    "# progress bar, changing the unit to bytes instead of iteration (default by tqdm)\n",
    "progress = tqdm(response.iter_content(buffer_size), f\"Downloading {filename}\", total=file_size, unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "\n",
    "with open(filename, \"wb\") as f:\n",
    "    for data in progress:\n",
    "        # write data read to the file\n",
    "        f.write(data)\n",
    "        # update the progress bar manually\n",
    "        progress.update(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import validators\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urlparse\n",
    "import wget\n",
    "from urllib.request import urlopen\n",
    "import urllib.request \n",
    "\n",
    "def check_validity(my_url):\n",
    "    try:\n",
    "        urlopen(my_url)\n",
    "        print(\"Valid URL\")\n",
    "    except IOError:\n",
    "        print (\"Invalid URL\")\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def get_pdfs(my_url):\n",
    "    links = []\n",
    "    html = urlopen(my_url).read()\n",
    "    html_page = bs(html, features=\"lxml\") \n",
    "    og_url = html_page.find(\"meta\",  property = \"og:url\")\n",
    "    base = urlparse(my_url)\n",
    "    print(\"base\",base)\n",
    "    for link in html_page.find_all('a'):\n",
    "        current_link = link.get('href')\n",
    "        if current_link.endswith('pdf'):\n",
    "            if og_url:\n",
    "                print(\"currentLink\",current_link)\n",
    "                links.append(og_url[\"content\"] + current_link)\n",
    "            else:\n",
    "                links.append(base.scheme + \"://\" + base.netloc + current_link)\n",
    "\n",
    "    for link in links:\n",
    "        try: \n",
    "            wget.download(link)\n",
    "        except:\n",
    "            print(\" \\n \\n Unable to Download A File \\n\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Enter Link: \")\n",
    "    my_url = input()\n",
    "    check_validity(my_url)\n",
    "    get_pdfs(my_url)\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "# https://grader.eecs.jacobs-university.de/courses/320241/2019_2\n",
    "# https://cnds.jacobs-university.de/courses/os-2019/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wget -r -P ./pdfs -A pdf http://kea.kar.nic.in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
