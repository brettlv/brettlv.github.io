{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "import gevent,re,time,os,glob\n",
    "from gevent import monkey\n",
    "monkey.patch_all()\n",
    "\n",
    "def geturllist(url):\n",
    "    url_list=[]\n",
    "    s=urllib.request.urlopen(url)\n",
    "    text=s.read()\n",
    "    html=re.search(r'<ol.*</ol>',text,re.S)\n",
    "    urls=re.finditer(r'<p><img src=\"(.+?)jpg\" /></p>',html.group(),re.I)\n",
    "    for i in urls:\n",
    "        url=i.group(1).strip()+str(\"jpg\")\n",
    "        url_list.append(url)\n",
    "    return url_list\n",
    "\n",
    "#设定抓取页数\n",
    "page_amount = 10\n",
    "\n",
    "#抓取首页的html代码   \n",
    "def get_page(url):\n",
    "    #req = urllib.request.Request(url)\n",
    "    webheader = {'User-Agent':'Mozilla/5.0'}\n",
    "   \n",
    "    req = urllib.request.Request(url=url, headers=webheader)  \n",
    "    response = urllib.request.urlopen(req)\n",
    "    html = response.read()\n",
    "    return html\n",
    "\n",
    "#抓取图片\n",
    "def read_image(url):\n",
    "    #req = urllib.request.Request(url)\n",
    "    webheader = {'User-Agent':'Mozilla/5.0'} \n",
    "    req = urllib.request.Request(url=url, headers=webheader) \n",
    "    response = urllib.request.urlopen(req)\n",
    "    html = response.read()\n",
    "    return html\n",
    "\n",
    "#得到当前的最新页面数，从这个页面开始倒着爬，因为用了这个脚本以后以前的图可能已经看过了\n",
    "def get_current_page_number(html):\n",
    "    match = re.search(r'<span class=\"current-comment-page\">\\[(.*)\\]</span>',html)\n",
    "    return match.group(1)\n",
    "\n",
    "#得到图片列表\n",
    "def get_picturs_url_list(url):\n",
    "    html = get_page(url)\n",
    "    l = re.findall(r'<p><img src=\"http://.*.sinaimg.cn/mw600/.*jpg\" /></p>',html)\n",
    "    result = []\n",
    "    for string in l:\n",
    "        src = re.search(r'\"(.*)\"',string)\n",
    "        result.append(str(src.group(1)))    #解决Unicode编码开头问题，有空好好补下编码和字符规范\n",
    "    return result\n",
    "\n",
    "#下载图片并存储到本地文件夹\n",
    "def image_save(url,number):\n",
    "    number = str(number)\n",
    "    #print '正在抓取第',number,'张'\n",
    "    filename = number + '.jpg'\n",
    "    with open(filename,'wb') as fp:\n",
    "        img = read_image(url)\n",
    "        fp.write(img)\n",
    "\n",
    "#准备存放图片的文件夹，并进入到指定路径\n",
    "os.chdir('/Users/brettlv/')\n",
    "os.mkdir('pic1')\n",
    "os.chdir(os.path.join(os.getcwd(), 'pic1'))\n",
    "\n",
    "#主函数\n",
    "def main1():\n",
    "    html = get_page('http://jandan.net/ooxx')\n",
    "    number = int(get_current_page_number(html))\n",
    "    l = []\n",
    "    amount = 0\n",
    "    for n in range(1,page_amount):\n",
    "        url = 'http://jandan.net/ooxx/page-' + str(n) + '#comments'\n",
    "        l += get_picturs_url_list(url) \n",
    "\n",
    "    for url in l:\n",
    "        amount += 1\n",
    "        image_save(url,amount)\n",
    "main1()\n",
    "print('全部抓完啦')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def fetch_pictures(url):\n",
    "    html_content = urllib.request.urlopen(url).read()\n",
    "    r = re.compile('<img class=\"BDE_Image\" pic_type=\"0\" width=\"\\d\\d\\d\" height=\"\\d\\d\\d\" src=\"(.*?)\"')\n",
    "    picture_url_list = r.findall(html_content.decode('utf-8'))\n",
    "\n",
    "    os.mkdir('longzhu')\n",
    "    os.chdir(os.path.join(os.getcwd(), 'longzhu'))\n",
    "    for i in range(len(picture_url_list)):\n",
    "        picture_name = str(i) + '.jpg'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(picture_url_list[i], picture_name)\n",
    "            print(\"Success to download \" + picture_url_list[i])\n",
    "        except:\n",
    "            print(\"Fail to download \" + picture_url_list[i])\n",
    "fetch_pictures(\"http://tieba.baidu.com/p/3720487356\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "os.chdir('/Users/brettlv/')\n",
    "os.mkdir('pic5')\n",
    "os.chdir(os.path.join(os.getcwd(), 'pic5'))\n",
    "\n",
    "url='http://tieba.baidu.com/p/2166231880'\n",
    "def fetch_pictures(url):\n",
    "    html_content = urllib.request.urlopen(url).read()\n",
    "    r = re.compile('<img class=\"BDE_Image\" pic_type=\"0\" width=\"\\d\\d\\d\" height=\"\\d\\d\\d\" src=\"(.*?)\"')\n",
    "    picture_url_list = r.findall(html_content.decode('utf-8'))\n",
    "\n",
    "    for i in range(len(picture_url_list)):\n",
    "        picture_name = str(i) + '.jpg'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(picture_url_list[i], picture_name)\n",
    "            print(\"Success to download \" + picture_url_list[i])\n",
    "        except:\n",
    "            print(\"Fail to download \" + picture_url_list[i])\n",
    "            \n",
    "fetch_pictures(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "os.chdir('/Users/brettlv/')\n",
    "\n",
    "def fetch_pictures(url):\n",
    "    html_content = urllib.request.urlopen(url).read()\n",
    "    r = re.compile('<img class=\"BDE_Image\" pic_type=\"\\d\" width=\"\\d\\d\\d\" height=\"\\d\\d\\d\" src=\"(.*?)\"')\n",
    "    picture_url_list = r.findall(html_content.decode('utf-8'))\n",
    "\n",
    "    os.mkdir('pic2')\n",
    "    os.chdir(os.path.join(os.getcwd(), 'pic2'))\n",
    "    for i in range(len(picture_url_list)):\n",
    "        picture_name = str(i) + '.jpg'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(picture_url_list[i], picture_name)\n",
    "            print(\"Success to download \" + picture_url_list[i])\n",
    "        except:\n",
    "            print(\"Fail to download \" + picture_url_list[i])\n",
    "fetch_pictures(\"http://tieba.baidu.com/p/3884688092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/brettlv/')\n",
    "def fetch_pictures(url):\n",
    "    html_content = urllib.request.urlopen(url).read()\n",
    "    r = re.compile('<img pic_type=\"0\" class=\"BDE_Image\" src=\"(.*?)\"')\n",
    "    picture_url_list = r.findall(html_content.decode('utf-8'))\n",
    "\n",
    "    os.mkdir('pic3')\n",
    "    os.chdir(os.path.join(os.getcwd(), 'pic3'))\n",
    "    for i in range(len(picture_url_list)):\n",
    "        picture_name = str(i) + '.jpg'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(picture_url_list[i], picture_name)\n",
    "            print(\"Success to download \" + picture_url_list[i])\n",
    "        except:\n",
    "            print(\"Fail to download \" + picture_url_list[i])\n",
    "fetch_pictures(\"http://tieba.baidu.com/p/2166231880\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#%run py3download.py #python py3download.py  in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
